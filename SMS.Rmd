---
title: "SMS"
output:
  word_document: default
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:
## Import all the libraries
```{r}
importlib <- c("ggplot2", "stringr", "magrittr", "futile.logger", "VennDiagram", "tm", "SnowballC", "wordcloud", "RColorBrewer", "lattice", "caret", "rpart", "rpart.plot", "randomForest", "e1071", "ROCR", "gmodels")

require(importlib)

lapply(importlib, require, character.only = TRUE)
```
## Download the dataset.
```{r}
Spam_SMS <- read.csv("./SMS_Spam_Dataset.csv", stringsAsFactors = F)
str(Spam_SMS)
```

## Clean the data. 

```{r}
#Remove Null Columns.
Spam_SMS$X <- NULL
Spam_SMS$X.1 <- NULL
Spam_SMS$X.2 <- NULL

#Assign appropriate names to the columns.
names(Spam_SMS) <- c("MessageLabel","Message")

#Check if any other NULL values exist in the dataset.
colSums(is.na(Spam_SMS))

#Convert class into factor.
levels(as.factor(Spam_SMS$MessageLabel))

#Assign appropriate names to the data entries under Column "Message_Label"
Spam_SMS$MessageLabel[Spam_SMS$MessageLabel == "ham"] <- "Legitimate"
Spam_SMS$MessageLabel[Spam_SMS$MessageLabel == "spam"] <- "Spam"

#Convert class into factor.
Spam_SMS$MessageLabel <- factor(Spam_SMS$MessageLabel)
```
#Explore the data

```{r}
#library(ggplot2)
#Explore the distribution of Legitimate and Spam messages.
Distribution <- as.data.frame(table(Spam_SMS$MessageLabel))

Distribution$Percentage <- (Distribution$Freq/nrow(Spam_SMS))*100
Distribution$Percentage <- round(Distribution$Percentage, digits = 2)
names(Distribution) <- c("Label", "Total", "Percentage")

#Plot the Distribution.
ggplot(Distribution, aes(x = Label, y = Percentage)) + geom_bar(stat= "identity", width = 0.5, fill = "dark green") + ylim(0,100) + labs(colour = "Percentage", title = "Distribution of Legitimate and Spam Messages") + geom_text(aes(label = Percentage), color = "red", vjust = 0) + xlab("Label")+ ylab("Percentage Value")
```

```{r}

#To know the length of each text so as to be able to explore the data more.
Spam_SMS$MessageLength <- nchar(Spam_SMS$Message)
summary(Spam_SMS$MessageLength)
```

```{r}
#Plot the distribution of Legitimate and Spammessages v/s the Message Length.
ggplot(Spam_SMS, aes(x = MessageLength, fill = MessageLabel)) +
  theme_bw() +
  geom_histogram(binwidth = 5) +
  labs(y = "Number of Messages", x = "Length of Message",
       title = "Distribution of Message Lengths with Class Labels")
```
Split Raw SMS Data on Labels (Spam and Legitmate) and produce wordclouds for each.

```{r}
Spam_Raw <- subset(Spam_SMS, MessageLabel == "Spam")
Legitimate_Raw <- subset(Spam_SMS, MessageLabel == "Legitimate")

#Wordcloud for Spam_Raw
pal = brewer.pal(6,"Dark2")
wordcloud(Spam_Raw$Message, max.words = 30, scale=c(8, .3), colors = pal)
```

```{r}
wordcloud(Legitimate_Raw$Message, max.words = 30, scale=c(1, .3), colors = pal)
```
## To convert all the tokens to lower case. Post that, I will run for loops for few words to see if these words are correctly assigned 'y' or 'n' for each message in the dataset. ('y' and 'n' correspond to availability of that word in a particular SMS.) 
```{r}
#library(stringr)
#library(magrittr)

#Transformation of all tokens to lower case.
Spam_SMS$Message %<>% str_to_lower()
Spam_SMS$free <- "n"
for(i in 1:nrow(Spam_SMS)){
  if(str_detect(Spam_SMS$Message[i], "free")  == TRUE){
    Spam_SMS$free[i] <- "y"
  }
}

Spam_SMS$winner <- "n"
for(i in 1:nrow(Spam_SMS)){
  if(str_detect(Spam_SMS$Message[i], "winner")  == TRUE){
    Spam_SMS$winner[i] <- "y"
  }
  if(str_detect(Spam_SMS$Message[i], "win")  == TRUE){
   Spam_SMS$winner[i] <- "y"
  }
  if(str_detect(Spam_SMS$Message[i], "won")  == TRUE){
   Spam_SMS$winner[i] <- "y"
  }
  if(str_detect(Spam_SMS$Message[i], "award")  == TRUE){
   Spam_SMS$winner[i] <- "y"
    }
    if(str_detect(Spam_SMS$Message[i], "selected")  == TRUE){
   Spam_SMS$winner[i] <- "y"
  }
}

Spam_SMS$congratulation <- "n"
for(i in 1:nrow(Spam_SMS)){
  if(str_detect(Spam_SMS$Message[i], "congrats")  == TRUE){
    Spam_SMS$congratulation[i] <- "y"
  }
  if(str_detect(Spam_SMS$Message[i], "congratulations")  == TRUE){
    Spam_SMS$congratulation[i] <- "y"
  }
}

Spam_SMS$adult <- "n"
for(i in 1:nrow(Spam_SMS)){
  if(str_detect(Spam_SMS$Message[i], "xxx")  == TRUE){
    Spam_SMS$adult[i] <- "y"
  }
  if(str_detect(Spam_SMS$Message[i], "babe")  == TRUE){
    Spam_SMS$adult[i] <- "y"
  }
  if(str_detect(Spam_SMS$Message[i], "naked")  == TRUE){
    Spam_SMS$adult[i] <- "y"
  }
    if(str_detect(Spam_SMS$Message[i], "dirty")  == TRUE){
    Spam_SMS$adult[i] <- "y"
    }
    if(str_detect(Spam_SMS$Message[i], "flirty")  == TRUE){
    Spam_SMS$adult[i] <- "y"
    }
}

Spam_SMS$attention <- "n"
for(i in 1:nrow(Spam_SMS)){
  if(str_detect(Spam_SMS$Message[i], "urgent")  == TRUE){
    Spam_SMS$attention[i] <- "y"
  }
    if(str_detect(Spam_SMS$Message[i], "attention")  == TRUE){
    Spam_SMS$attention[i] <- "y"
    }
    if(str_detect(Spam_SMS$Message[i], "bonus")  == TRUE){
    Spam_SMS$attention[i] <- "y"
      }
    if(str_detect(Spam_SMS$Message[i], "immediately")  == TRUE){
    Spam_SMS$attention[i] <- "y"
  }
}

Spam_SMS$ringtone  <- "n"
for(i in 1:nrow(Spam_SMS)){
  if(str_detect(Spam_SMS$Message[i], "ringtone")  == TRUE){
    Spam_SMS$ringtone[i] <- "y"
  }
}

```

##Plot bar graph depicting total number of messages with the value of these features being equal to "y".
```{r}
#For Unigrams

Spam_Features <- data.frame(Features = c("Free", "Adult", "Ringtone", "Congratulation", "Winner", "Attention"), Total = c(sum(Spam_SMS$free == "y"), sum(Spam_SMS$adult == "y"), sum(Spam_SMS$ringtone == "y"), sum(Spam_SMS$congratulation == "y"), sum(Spam_SMS$winner == "y"), sum(Spam_SMS$attention == "y")))

ggplot(Spam_Features, aes(x = reorder(Features, -Total), y = Total)) + geom_bar(stat = "identity", fill = "steelblue") + geom_text(aes(label = Total), color = "red", vjust = 0) + xlab("Features")+ ylab("Total")
```
##Produce Venn Diagram to see how many SMS messages have combination of these features (bigrams and trigrams feature combinations).
```{r}
#library(futile.logger)
#library(VennDiagram)
#I will start by computing the number of SMS messages having combination of two and/or three features. After having obtained these values, I will produce Venn Diagrams for these combinations.

#For bigrams

#For Free and Adult
Free_Adult <- sum(Spam_SMS$free == "y" & Spam_SMS$adult == "y")
Free_Adult
grid.newpage()
draw.pairwise.venn(area1 = 265, area2 = 150, cross.area = 9, category = c("Free", 
    "Adult"), lty = rep("blank", 
    2), fill = c("light blue", "pink"), alpha = rep(0.5, 2), cat.pos = c(0, 
    0), cat.dist = rep(0.025, 2))

#For Free and Ringtone
Free_Ringtone <- sum(Spam_SMS$free == "y" & Spam_SMS$ringtone == "y")
Free_Ringtone
grid.newpage()
draw.pairwise.venn(area1 = 265, area2 = 40, cross.area = 14, category = c("Free", 
    "Ringtone"), lty = rep("blank", 
    2), fill = c("light blue", "pink"), alpha = rep(0.5, 2), cat.pos = c(0, 
    0), cat.dist = rep(0.025, 2))

#For Free and Congratulation
Free_Congratulation <- sum(Spam_SMS$free == "y" & Spam_SMS$congratulation == "y")
Free_Congratulation
grid.newpage()
draw.pairwise.venn(area1 = 265, area2 = 34, cross.area = 9, category = c("Free", 
    "Congratulation"), lty = rep("blank", 
    2), fill = c("light blue", "pink"), alpha = rep(0.5, 2), cat.pos = c(0, 
    0), cat.dist = rep(0.025, 2))

#For Free and Winner
Free_Winner <- sum(Spam_SMS$free == "y" & Spam_SMS$winner == "y")
Free_Winner
grid.newpage()
draw.pairwise.venn(area1 = 265, area2 = 368, cross.area = 39, category = c("Free", 
    "Winner"), lty = rep("blank", 
    2), fill = c("light blue", "pink"), alpha = rep(0.5, 2), cat.pos = c(0, 
    0), cat.dist = rep(0.025, 2))

#For Free and Attention
Free_Attention <- sum(Spam_SMS$free == "y" & Spam_SMS$attention == "y")
Free_Attention
grid.newpage()
draw.pairwise.venn(area1 = 265, area2 = 85, cross.area = 3, category = c("Free", 
    "Attention"), lty = rep("blank", 
    2), fill = c("light blue", "pink"), alpha = rep(0.5, 2), cat.pos = c(0, 
    0), cat.dist = rep(0.025, 2))

#For Adult and Winner
Adult_Winner <- sum(Spam_SMS$adult == "y" & Spam_SMS$winner == "y")
Adult_Winner
grid.newpage()
draw.pairwise.venn(area1 = 150, area2 = 368, cross.area = 8, category = c("Adult", 
    "Winner"), lty = rep("blank", 
    2), fill = c("light blue", "pink"), alpha = rep(0.5, 2), cat.pos = c(0, 
    0), cat.dist = rep(0.025, 2))

#For Adult and Attention
Adult_Attention <- sum(Spam_SMS$adult == "y" & Spam_SMS$attention == "y")
Adult_Attention
grid.newpage()
draw.pairwise.venn(area1 = 150, area2 = 85, cross.area = 3, category = c("Adult", 
    "Attention"), lty = rep("blank", 
    2), fill = c("light blue", "pink"), alpha = rep(0.5, 2), cat.pos = c(0, 
    0), cat.dist = rep(0.025, 2))

#For Congratulation and Winner
congratulation_Winner <- sum(Spam_SMS$congratulation == "y" & Spam_SMS$winner == "y")
congratulation_Winner
grid.newpage()
draw.pairwise.venn(area1 = 34, area2 = 368, cross.area = 13, category = c("Congratulation", 
    "Winner"), lty = rep("blank", 
    2), fill = c("light blue", "pink"), alpha = rep(0.5, 2), cat.pos = c(0, 
    0), cat.dist = rep(0.025, 2))

#For Attention and Winner
Attention_Winner <- sum(Spam_SMS$attention == "y" & Spam_SMS$winner == "y")
Attention_Winner
grid.newpage()
draw.pairwise.venn(area1 = 85, area2 = 368, cross.area = 47, category = c("Attention", 
    "Winner"), lty = rep("blank", 
    2), fill = c("light blue", "pink"), alpha = rep(0.5, 2), cat.pos = c(0, 
    0), cat.dist = rep(0.025, 2))
```

```{r}
#For trigrams

#For free, congratulation and winner 
Free_Congratulation_Winner <- sum(Spam_SMS$free == "y" & Spam_SMS$congratulation == "y" & Spam_SMS$winner == "y")
Free_Congratulation_Winner
grid.newpage()
draw.triple.venn(area1 = 265, area2 = 34, area3 = 368, n12 = 9, n23 = 13, n13 = 39, 
    n123 = 6, category = c("Free", "Congratulation", "Winner"), lty = "blank", 
    fill = c("skyblue", "pink1", "mediumorchid"))

#For free, attention and winner 
Free_Attention_Winner <- sum(Spam_SMS$free == "y" & Spam_SMS$attention == "y" & Spam_SMS$winner == "y")
Free_Attention_Winner
grid.newpage()
draw.triple.venn(area1 = 265, area2 = 85, area3 = 368, n12 = 3, n23 = 47, n13 = 39, 
    n123 = 2, category = c("Free", "Attention", "Winner"), lty = "blank", 
    fill = c("skyblue", "pink1", "mediumorchid"))

#For adult, attention and winner 
Adult_Attention_Winner <- sum(Spam_SMS$adult == "y" & Spam_SMS$attention == "y" & Spam_SMS$winner == "y")
Adult_Attention_Winner
grid.newpage()
draw.triple.venn(area1 = 150, area2 = 85, area3 = 368, n12 = 3, n23 = 47, n13 = 8, 
    n123 = 3, category = c("Adult", "Attention", "Winner"), lty = "blank", 
    fill = c("skyblue", "pink1", "mediumorchid"))
```

##To make the data ready for text analysis. In this, we use text-mining package (package tm) to manage the documents.
```{r}
#library(tm)
#library(SnowballC)
#library(wordcloud)
#library(RColorBrewer)
#library(magrittr)

BagOfWords <- Corpus(VectorSource(Spam_SMS$Message))

#To clean corpus.
Clean_BagOfWords <- BagOfWords %>%
                    tm_map(content_transformer(tolower)) %>% #Transofrm to lower case
                    tm_map(removeNumbers) %>%                #Clean by removing numbers
                    tm_map(removeWords, stopwords(kind="en")) %>% #Clean by removing stopwords
                    tm_map(removePunctuation) %>%            #Clean by removing punctuation
                    tm_map(stripWhitespace)                  #Clean by tokenising by striping white space

#Transform corpus into matrix.
TDM = DocumentTermMatrix(Clean_BagOfWords)

SparseWords <- removeSparseTerms(TDM, 0.995)

# convert the matrix of sparse words to data frame
SparseWords <- as.data.frame(as.matrix(SparseWords))

# rename column names to proper format in order to be used by R
colnames(SparseWords) <- make.names(colnames(SparseWords))

str(SparseWords)

SparseWords$MessageLabel <- Spam_SMS$MessageLabel

```
Preparing prediction models to classify SMS messages as Spam or Legitimate.

## Splitting the data in a ratio of 7:3: 70% to build the predictive model and 30% to test the model. I am splitting the dataset, Corpus(BagOfWords) and the Term Document Matrix(FinalData). 
```{r}
#library(lattice)
#library(caret)
#Random number generation using set.seed of 1234.
set.seed(1234)

#To create a split formula using which I would split the data into train and test sets.
Split_Formula <- createDataPartition(Spam_SMS$MessageLabel, p=0.7, list=FALSE)

#To split Spam_SMS into training and test sets.
train_data <- Spam_SMS[Split_Formula,]
test_data <- Spam_SMS[-Split_Formula,]

#To split SparseWords into training and test sets.
Sparse_train_data <- SparseWords[Split_Formula,]
Sparse_test_data <- SparseWords[-Split_Formula,]

#To split corpus into training and test data.
Corpus_train_data <- Clean_BagOfWords[Split_Formula]
Corpus_test_data <- Clean_BagOfWords[-Split_Formula]

#To split Term Document Matrix into training and test data.
TDM_train_data <- TDM[Split_Formula,]
TDM_test_data <- TDM[-Split_Formula,]


```

##Producing Wordclouds
```{r}
#Using Wordcloud to see frequent words. More the word is frequent, larger the font will be for it. 
wordcloud(Clean_BagOfWords, max.words = 75, random.order = FALSE, scale=c(5, .3), colors = pal)
```

```{r}
#Producing separate wordclouds for Legitimate and Spam messages of train_data.
Spam <- subset(train_data, MessageLabel == "Spam")
Legitimate <- subset(train_data, MessageLabel == "Legitimate")

#Wordcloud for Spam
wordcloud(Spam$Message, max.words = 30, scale=c(7, .3), colors = pal)
```

```{r}
#Wordcloud for Legitimate
wordcloud(Legitimate$Message, max.words = 30, scale=c(5, .3), colors = pal)
```

```{r}
#To build a recursive partitioning decision tree.
library(rpart)
library(rpart.plot)
SMS_Rpart <- rpart(formula = MessageLabel ~ free + winner + congratulation + adult + attention + ringtone, data = train_data, method = "class")

rpart.plot(SMS_Rpart, type = 4, fallen.leaves = FALSE, extra = 4)
```
##This tree reveals that out of all these tokens, the most important token is 'free' and the least important ones being 'adult, ringtone and congratulation'.
```{r}
summary(SMS_Rpart)
```
##Applying Random Forest to plot the importance of each token.
```{r}
#library(randomForest)

train_data$MessageLabel %<>% as.factor()
train_data$Message  %<>% as.character()
train_data$free %<>% as.factor()
train_data$winner %<>% as.factor()
train_data$congratulation %<>% as.factor()
train_data$adult  %<>% as.factor()
train_data$attention   %<>% as.factor()
train_data$ringtone %<>% as.factor()

#Applying the formula for Random Forest Algorithm
SMS_RF <- MessageLabel ~ free + winner + congratulation + adult + attention + ringtone
RFSpam_Tree <- randomForest(SMS_RF, data = train_data, ntree=25, proximity = T)

#To plot the Variable Importance Plot.
ImportancePlot <- varImpPlot(RFSpam_Tree, main = "Importance of each Token") 

```
This plot also expresses that the most important token aongst all is 'free', and the least important are 'adult, congratulation and ringtone'.

```{r}
#Importance of each token in a tabular form.
importance(RFSpam_Tree)
```

#To test the above Random Forest Model on test data and check the prediction percentage.
```{r}
test_data$MessageLabel %<>% as.factor()
test_data$Message  %<>% as.character()
test_data$free %<>% as.factor()
test_data$winner %<>% as.factor()
test_data$congratulation %<>% as.factor()
test_data$adult  %<>% as.factor()
test_data$attention   %<>% as.factor()
test_data$ringtone %<>% as.factor()


RFTest <- predict(RFSpam_Tree, newdata =test_data)
TestPredictionTable <- table(RFTest, test_data$MessageLabel)
print(TestPredictionTable)
```

Confusion Matrix
```{r}
Measure <- confusionMatrix(predict(RFSpam_Tree, newdata = test_data), test_data$MessageLabel)
Measure$byClass[5:7]
```
##Prediction percentage for test data.
```{r}
TestPredictability <- sum(RFTest == test_data$MessageLabel)/ length(test_data$MessageLabel)*100
message("Predcitability Percentage for Test Data is:")
print(TestPredictability)
```
##Producing Heatmap for the above model.
```{r}
library(lattice)
library(caret)
#Converting table for 70-30 split regime into a data frame.
RFSpam_Tree_Table <- as.data.frame(table(predict(RFSpam_Tree, newdata = test_data), test_data$MessageLabel))

#Normalizing data between 0 and 1 using Min-Max Normalization. 
Normalized = (RFSpam_Tree_Table$Freq-min(RFSpam_Tree_Table$Freq))/(max(RFSpam_Tree_Table$Freq) - min(RFSpam_Tree_Table$Freq))
#Replacing data of Freq column with Normalized data.
RFSpam_Tree_Table[,"Freq"] <- Normalized

# ggplotting the contents of table of 70-30 split regime into Heatmap.
Heatmap <- ggplot(RFSpam_Tree_Table)

#Final Plotting step
Heatmap + geom_tile(aes(x=Var1, y=Var2, fill=Freq), colour = "white") + scale_x_discrete(name="Reference Class",  position = "top") + scale_y_discrete(name="Predicted Class", limits = rev(levels(RFSpam_Tree_Table$Var1))) + scale_fill_gradient(breaks=seq(from=-.5, to=4, by=.25), low = "white", high = "steelblue") + labs(fill="Normalized\nFrequency")


```



##Train Support Vector Machine
```{r}
#library(e1071)

SMS_SVM <- svm(MessageLabel ~ free + winner + congratulation + adult + attention + ringtone, data = train_data, kernel = "linear", cost = 0.1, gamma = 0.1)
SVMTest <- predict(SMS_SVM, test_data)

SVM_Measure <- confusionMatrix(predict(SMS_SVM, newdata = test_data), test_data$MessageLabel)
SVM_Measure$byClass[5:7]
```

##Producing heatmaps for the above confusion matrix.
```{r}
SVM_Tree <- as.data.frame(table(predict(SMS_SVM, newdata = test_data), test_data$MessageLabel)) 
#Normalizing data between 0 and 1 using Min-Max Normalization. 
Normalized = (SVM_Tree$Freq-min(SVM_Tree$Freq))/(max(SVM_Tree$Freq) - min(SVM_Tree$Freq))
#Replacing data of Freq column with Normalized data.
SVM_Tree[,"Freq"] <- Normalized
#ggplotting the contents of table of 70-30 split regime into Heatmap.
Heatmap <- ggplot(SVM_Tree)
 
#Final Plotting step
Heatmap + geom_tile(aes(x=Var1, y=Var2, fill=Freq), colour = "white") + scale_x_discrete(name="Reference Class",  position = "top") + scale_y_discrete(name="Predicted Class", limits = rev(levels(SVM_Tree$Var1))) + scale_fill_gradient(breaks=seq(from=-.5, to=4, by=.25), low = "white", high = "steelblue") + labs(fill="Normalized\nFrequency")
```

```{r}
svm.accuracy.table <- as.data.frame(table(test_data$MessageLabel, SVMTest))
print(paste("Accuracy for SVM is:",
            100*round(((svm.accuracy.table$Freq[1]+svm.accuracy.table$Freq[4])/nrow(test_data)), 4),
            "%"))

```

## Logistic regression

```{r}
SMS_GLM <- glm(MessageLabel ~ free + winner + congratulation + adult + attention + ringtone, data = train_data, family = "binomial")
GLMTest <- predict(SMS_GLM, test_data, type = 'response')

#Confusion Matrix
table(test_data$MessageLabel, GLMTest > 0.5)
```

```{r}
library(ROCR)
Logistic_Regression_Prediction <- prediction(abs(GLMTest), test_data$MessageLabel)
Logistic_Regression_Performance <- performance(Logistic_Regression_Prediction,"tpr","fpr")
plot(Logistic_Regression_Performance, colorize = TRUE, text.adj = c(-0.2,1.7))
```

```{r}
#table(test_data$Label, Logistic_Regression_Test > 0.75)
glm.accuracy.table <- as.data.frame(table(test_data$MessageLabel, GLMTest > 0.75))
print(paste("Accuracy of Logistic Regression is:",
            100*round(((glm.accuracy.table$Freq[1]+glm.accuracy.table$Freq[4])/nrow(test_data)), 4),
            "%"))
```

##Naive Bayes Classifier
```{r}
#Retain words which appear in 5 or more than 5 SMS messages.
Frequent_Terms = findFreqTerms(TDM_train_data, 5)
TDM_train_data_New = DocumentTermMatrix(Corpus_train_data, list(dictionary=Frequent_Terms))
TDM_test_data_New =  DocumentTermMatrix(Corpus_test_data, list(dictionary=Frequent_Terms))
```

```{r}
#To write a function to convert numerics in TDms to factors of yes/no.
Convert_Numerics_To_Factors = function(num) 
  {
  num = ifelse(num > 0, 1, 0)
  num = factor(num, levels = c(0, 1), labels=c("No", "Yes"))
  return (num)
  }

#Apply above fucntion to the new TDM train and test datasets.
TDM_train_data_New = apply(TDM_train_data_New, MARGIN=2, Convert_Numerics_To_Factors)
TDM_test_data_New  = apply(TDM_test_data_New, MARGIN=2, Convert_Numerics_To_Factors)
```

```{r}
#Train a Naive Bayes Model.

#library(e1071)
# store our model in sms_classifier
SMS_NB = naiveBayes(MessageLabel ~ free + winner + congratulation + adult + attention + ringtone, data = train_data, laplace = 1)
SMS_NBTest = predict(SMS_NB, TDM_test_data_New)


library(gmodels)
CT <- CrossTable(SMS_NBTest, test_data$MessageLabel, 
           prop.chisq = FALSE, 
           prop.t = FALSE, 
           dnn = c("Predicted", "Actual")) #Name of column
```
Logistic Regression
```{r}
SMS_GLM_Blah <- glm(MessageLabel ~., data = Sparse_train_data, family = "binomial")
GLMTest_Blah <- predict(SMS_GLM_Blah, Sparse_test_data, type = 'response')

#Confusion Matrix
table(Sparse_test_data$MessageLabel, GLMTest_Blah > 0.5)
```

```{r}
library(ROCR)
Logistic_Regression_Prediction_Blah <- prediction(abs(GLMTest_Blah), Sparse_test_data$MessageLabel)
Logistic_Regression_Performance_Blah <- performance(Logistic_Regression_Prediction_Blah,"tpr","fpr")

probab.cuts <- data.frame(cut=Logistic_Regression_Performance_Blah@alpha.values[[1]], prec=Logistic_Regression_Performance_Blah@y.values[[1]], rec=Logistic_Regression_Performance_Blah@x.values[[1]])
tail(probab.cuts[probab.cuts$cut > 0.5,], 1)
plot(Logistic_Regression_Performance_Blah, colorize = TRUE, text.adj = c(-0.2,1.7))
```

