---
title: "SMS"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

## To download the dataset.
```{r}
Spam_SMS <- read.csv("C:/Users/Harshita Jain/Desktop/SMS_Spam_Dataset.csv", stringsAsFactors = F)
str(Spam_SMS)
```

## Clean the data. 

```{r}
#Remove Null Columns.
Spam_SMS$X <- NULL
Spam_SMS$X.1 <- NULL
Spam_SMS$X.2 <- NULL

#Assign appropriate names to the columns.
names(Spam_SMS) <- c("MessageLabel","Message")

#Check if there exists any other NULL values in the dataset.
colSums(is.na(Spam_SMS))

#Convert class into factor.
levels(as.factor(Spam_SMS$MessageLabel))

#Assign appropriate names to the data entries under Column "Message_Label"
Spam_SMS$MessageLabel[Spam_SMS$MessageLabel == "ham"] <- "Legitimate"
Spam_SMS$MessageLabel[Spam_SMS$MessageLabel == "spam"] <- "Spam"

#Convert class into factor.
Spam_SMS$MessageLabel <- factor(Spam_SMS$MessageLabel)
```
##Explore the data

```{r}
#Explore the distribution of Legitimate and Spam messages.
library(dplyr)
Distribution <- tally(group_by(Spam_SMS, MessageLabel))
names(Distribution) <- c("Label", "Total")
Distribution$Percentage <- (Distribution$Total)/(sum(Distribution$Total))

#Plot the distribution
library(ggplot2)
ggplot(count, aes(x = Label, fill = Percentage)) + theme_bw() + geom_bar() + labs(y = "Percentage Distribution", x = "Message Label", title = "Distribution of Message Labels")
```
```{r}

#To know the length of each text so as to be able to explore the data more.
Spam_SMS$MessageLength <- nchar(Spam_SMS$Message)
summary(Spam_SMS$MessageLength)

#Plot the distribution of Legitimate and Spammessages v/s the Message Length.
ggplot(Spam_SMS, aes(x = MessageLength, fill = MessageLabel)) +
  theme_bw() +
  geom_histogram(binwidth = 5) +
  labs(y = "Number of Messages", x = "Length of Message",
       title = "Distribution of Message Lengths with Class Labels")
```
## To convert all the tokens to lower case. Post that, I will run for loops for few words to see if these words are correctly assigned 'y' or 'n' for each message in the dataset. ('y' and 'n' correspond to availability of that word in a particular SMS.) 
```{r}
library(stringr)

#Transformation of all tokens to lower case.
Spam_SMS$Message %<>% str_to_lower()
Spam_SMS$free <- "n"
for(i in 1:nrow(Spam_SMS)){
  if(str_detect(Spam_SMS$Message[i], "free")  == TRUE){
    Spam_SMS$free[i] <- "y"
  }
}

Spam_SMS$winner <- "n"
for(i in 1:nrow(Spam_SMS)){
  if(str_detect(Spam_SMS$Message[i], "winner")  == TRUE){
    Spam_SMS$winner[i] <- "y"
  }
  if(str_detect(Spam_SMS$Message[i], "win")  == TRUE){
   Spam_SMS$winner[i] <- "y"
  }
  if(str_detect(Spam_SMS$Message[i], "won")  == TRUE){
   Spam_SMS$winner[i] <- "y"
  }
  if(str_detect(Spam_SMS$Message[i], "award")  == TRUE){
   Spam_SMS$winner[i] <- "y"
    }
    if(str_detect(Spam_SMS$Message[i], "selected")  == TRUE){
   Spam_SMS$winner[i] <- "y"
  }
}

Spam_SMS$congratulation <- "n"
for(i in 1:nrow(Spam_SMS)){
  if(str_detect(Spam_SMS$Message[i], "congrats")  == TRUE){
    Spam_SMS$congratulation[i] <- "y"
  }
  if(str_detect(Spam_SMS$Message[i], "congratulations")  == TRUE){
    Spam_SMS$congratulation[i] <- "y"
  }
}

Spam_SMS$adult <- "n"
for(i in 1:nrow(Spam_SMS)){
  if(str_detect(Spam_SMS$Message[i], "xxx")  == TRUE){
    Spam_SMS$adult[i] <- "y"
  }
  if(str_detect(Spam_SMS$Message[i], "babe")  == TRUE){
    Spam_SMS$adult[i] <- "y"
  }
  if(str_detect(Spam_SMS$Message[i], "naked")  == TRUE){
    Spam_SMS$adult[i] <- "y"
  }
    if(str_detect(Spam_SMS$Message[i], "dirty")  == TRUE){
    Spam_SMS$adult[i] <- "y"
    }
    if(str_detect(Spam_SMS$Message[i], "flirty")  == TRUE){
    Spam_SMS$adult[i] <- "y"
    }
}

Spam_SMS$attention <- "n"
for(i in 1:nrow(Spam_SMS)){
  if(str_detect(Spam_SMS$Message[i], "urgent")  == TRUE){
    Spam_SMS$attention[i] <- "y"
  }
    if(str_detect(Spam_SMS$Message[i], "attention")  == TRUE){
    Spam_SMS$attention[i] <- "y"
    }
    if(str_detect(Spam_SMS$Message[i], "bonus")  == TRUE){
    Spam_SMS$attention[i] <- "y"
      }
    if(str_detect(Spam_SMS$Message[i], "immediately")  == TRUE){
    Spam_SMS$attention[i] <- "y"
  }
}

Spam_SMS$ringtone  <- "n"
for(i in 1:nrow(Spam_SMS)){
  if(str_detect(Spam_SMS$Message[i], "ringtone")  == TRUE){
    Spam_SMS$ringtone[i] <- "y"
  }
}
```


## Splitting the data in a ratio of 7:3: 70% to build the predictive model and 30% to test the model.
```{r}
#Random number generation using set.seed of 1234.
set.seed(1234)

#Splitting the data into training data and test data.
SMS <- sample(2, nrow(Spam_SMS), replace=TRUE, prob=c(0.7, 0.3))
train_data <- Spam_SMS[SMS==1,]
test_data <- Spam_SMS[SMS==2,]

#To build a recursive partitioning decision tree.
library(rpart)
library(rpart.plot)
SMS_Rpart <- rpart(formula = MessageLabel ~ free + winner + congratulation + adult + attention + ringtone, data = train_data, method = "class")

rpart.plot(SMS_Rpart, type = 4, fallen.leaves = FALSE, extra = 4)
```
##This tree reveals that out of all these tokens, the most important token is 'free' and the least important ones being 'adult, ringtone and congratulation'.
```{r}
summary(SMS_Rpart)
```
##Applying Random Forest to plot the importance of each token.
```{r}
library(randomForest)

train_data$MessageLabel %<>% as.factor()
train_data$Message  %<>% as.character()
train_data$free %<>% as.factor()
train_data$winner %<>% as.factor()
train_data$congratulation %<>% as.factor()
train_data$adult  %<>% as.factor()
train_data$attention   %<>% as.factor()
train_data$ringtone %<>% as.factor()

#Applying the formula for Random Forest Algorithm
SMS_RF <- MessageLabel ~ free + winner + congratulation + adult + attention + ringtone
RFSpam_Tree <- randomForest(SMS_RF, data = train_data, ntree=25, proximity = T)

#To plot the Variable Importance Plot.
ImportancePlot <- varImpPlot(RFSpam_Tree, main = "Importance of each Token") 

```
This plot also expresses that the most important token aongst all is 'free', and the least important are 'adult, congratulation and ringtone'.

```{r}
#Importance of each token in a tabular form.
importance(RFSpam_Tree)
```

#To test the above Random Forest Model on test data and check the prediction percentage.
```{r}
test_data$MessageLabel %<>% as.factor()
test_data$Message  %<>% as.character()
test_data$free %<>% as.factor()
test_data$winner %<>% as.factor()
test_data$congratulation %<>% as.factor()
test_data$adult  %<>% as.factor()
test_data$attention   %<>% as.factor()
test_data$ringtone %<>% as.factor()


RFTest <- predict(RFSpam_Tree, newdata =test_data)
TestPredictionTable <- table(RFTest, test_data$MessageLabel)
print(TestPredictionTable)
```
##Prediction percentage for test data.
```{r}
TestPredictability <- sum(RFTest == test_data$MessageLabel)/ length(test_data$MessageLabel)*100
message("Predcitibility Percentage for Test Data is:")
print(TestPredictability)
```

##To make the data ready for text analysis. In this, we use text-mining package (package tm) to manage the documents.
```{r}
library(tm)
library(SnowballC)
BagOfWords <- Corpus(VectorSource(Spam_SMS$Message))

#To perform transformations on words to make them converted to lower case, remove punctuation and white space, stopwords and to perform stemming. 
FinalData = TermDocumentMatrix(BagOfWords, 
                       control = list(tolower = TRUE,
                                      removePunctuation = TRUE, 
                                      PlainTextDocument = TRUE,
                                      stopwords =  TRUE, 
                                      stripWhitespace = TRUE, stemDocument = TRUE)) 
```


```{r}
#To make a document-term matrix to record the frequency of all terms that are there in the whole collection.  
FrequencyOfTerms <- DocumentTermMatrix(BagOfWords)

#To find those terms that appear atleast 300 times in the collection.
findFreqTerms(frequencies, lowfreq = 300)
```

```{r}

sparseWords <- removeSparseTerms(FrequencyOfTerms, 0.995)

#To convert matrix of sparse words into a data frame.
sparseWords <- as.data.frame(as.matrix(sparseWords))

colnames(sparseWords) <- make.names(colnames(sparseWords))

str(sparseWords)
```

```{r}
sparseWords$Label <- Spam_SMS$Message_Label
```

```{r}
set.seed(987)

#SMS <- sample(2, nrow(sparseWords), replace=TRUE, prob=c(0.75, 0.25))
#train_data <- sparseWords[SMS==1,]
#test_data <- sparseWords[SMS==2,]
library(caTools)
# Split the data into 75% training and 25% test data.
split <- sample.split(sparseWords$Label, SplitRatio = 0.75)
train_data <- subset(sparseWords, split == T)
test_data <- subset(sparseWords, split == F)
```

```{r}

library(rpart)
library(rpart.plot)

#To build a recursive partitioning  tree.
Decision_Tree_Formula <- rpart(Label ~ ., data = train_data, method = "class", minbucket = 35)

prp(Decision_Tree_Formula)

 
```


```{r}
#To apply the above created model on test data.
Decision_Tree_Test_Model <- predict(Decision_Tree_Formula, test_data, type = "class")
table(test_data$Label, Decision_Tree_Test_Model)

rpart.accuracy.table <- as.data.frame(table(test_data$Label, Decision_Tree_Test_Model))
print(paste("Accuracy for Decision Tree is:",
            100*round(((rpart.accuracy.table$Freq[1]+rpart.accuracy.table$Freq[4])/nrow(test_data)), 4),
            "%"))
```



##SVM

```{r}
library(e1071)

#To build a Support Vetor Model for this dataset.
Support_Vector_Machine_Formula <- svm(Label ~ ., data = train_data, kernel = "linear", cost = 0.1, gamma = 0.1)
```

```{r}
#To apply the above created model on test data.
Support_Vector_Machine_Test <- predict(Support_Vector_Machine_Formula, test_data)
table(test_data$Label, Support_Vector_Machine_Test)

svm.accuracy.table <- as.data.frame(table(test_data$Label, Support_Vector_Machine_Test))
print(paste("Accuracy for SVM is:",
            100*round(((svm.accuracy.table$Freq[1]+svm.accuracy.table$Freq[4])/nrow(test_data)), 4),
            "%"))

```

## Logistic regression

```{r}
#To build a Support Vetor Model for this dataset.
Logistic_Regression_Formula <- glm(Label ~ ., data = train_data, family = "binomial")

#To apply the above created model on test data.
Logistic_Regression_Test <- predict(Logistic_Regression_Formula, test_data, type = "response")


library(ROCR)
Logistic_Regression_ROCR_Curve <- prediction(Logistic_Regression_Test, test_data$Label)
print(Logistic_Regression_AUC <- as.numeric(performance(Logistic_Regression_ROCR_Curve,"auc")@y.values))
```

```{r}
Logistic_Regression_Prediction <- prediction(abs(Logistic_Regression_Test), test_data$Label)
Logistic_Regression_Performance <- performance(Logistic_Regression_Prediction,"tpr","fpr")
plot(Logistic_Regression_Performance)
```

```{r}
table(test_data$Label, Logistic_Regression_Test > 0.75)
glm.accuracy.table <- as.data.frame(table(test_data$Label, Logistic_Regression_Test > 0.75))
print(paste("Accuracy of Logistic Regression is:",
            100*round(((glm.accuracy.table$Freq[1]+glm.accuracy.table$Freq[4])/nrow(test_data)), 4),
            "%"))
```