---
title: "SMS"
output:
  word_document: default
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

## To download the dataset.
```{r}
Spam_SMS <- read.csv("./SMS_Spam_Dataset.csv", stringsAsFactors = F)
str(Spam_SMS)
```

## Clean the data. 

```{r}
#Remove Null Columns.
Spam_SMS$X <- NULL
Spam_SMS$X.1 <- NULL
Spam_SMS$X.2 <- NULL

#Assign appropriate names to the columns.
names(Spam_SMS) <- c("MessageLabel","Message")

#Check if there exists any other NULL values in the dataset.
colSums(is.na(Spam_SMS))

#Convert class into factor.
levels(as.factor(Spam_SMS$MessageLabel))

#Assign appropriate names to the data entries under Column "Message_Label"
Spam_SMS$MessageLabel[Spam_SMS$MessageLabel == "ham"] <- "Legitimate"
Spam_SMS$MessageLabel[Spam_SMS$MessageLabel == "spam"] <- "Spam"

#Convert class into factor.
Spam_SMS$MessageLabel <- factor(Spam_SMS$MessageLabel)
```
##Explore the data

```{r}
library(ggplot2)
#Explore the distribution of Legitimate and Spam messages.
Distribution <- table(Spam_SMS$MessageLabel)

Percentage <- as.vector(Distribution)/nrow(Spam_SMS)

ggplot(data = Spam_SMS,aes(x=MessageLabel, colour = (..count..)/sum(..count..) )) + geom_bar(aes(y = (..count..)/sum(..count..)*100)) + ylab("Percentage") + xlab("Label") + ylim(0,100) + labs(colour = "Percentage", title = "Percentage of Legitimate and Spam Messages")
```

```{r}

#To know the length of each text so as to be able to explore the data more.
Spam_SMS$MessageLength <- nchar(Spam_SMS$Message)
summary(Spam_SMS$MessageLength)
```

```{r}
#Plot the distribution of Legitimate and Spammessages v/s the Message Length.
ggplot(Spam_SMS, aes(x = MessageLength, fill = MessageLabel)) +
  theme_bw() +
  geom_histogram(binwidth = 5) +
  labs(y = "Number of Messages", x = "Length of Message",
       title = "Distribution of Message Lengths with Class Labels")
```

## To convert all the tokens to lower case. Post that, I will run for loops for few words to see if these words are correctly assigned 'y' or 'n' for each message in the dataset. ('y' and 'n' correspond to availability of that word in a particular SMS.) 
```{r}
library(stringr)
library(magrittr)

#Transformation of all tokens to lower case.
Spam_SMS$Message %<>% str_to_lower()
Spam_SMS$free <- "n"
for(i in 1:nrow(Spam_SMS)){
  if(str_detect(Spam_SMS$Message[i], "free")  == TRUE){
    Spam_SMS$free[i] <- "y"
  }
}

Spam_SMS$winner <- "n"
for(i in 1:nrow(Spam_SMS)){
  if(str_detect(Spam_SMS$Message[i], "winner")  == TRUE){
    Spam_SMS$winner[i] <- "y"
  }
  if(str_detect(Spam_SMS$Message[i], "win")  == TRUE){
   Spam_SMS$winner[i] <- "y"
  }
  if(str_detect(Spam_SMS$Message[i], "won")  == TRUE){
   Spam_SMS$winner[i] <- "y"
  }
  if(str_detect(Spam_SMS$Message[i], "award")  == TRUE){
   Spam_SMS$winner[i] <- "y"
    }
    if(str_detect(Spam_SMS$Message[i], "selected")  == TRUE){
   Spam_SMS$winner[i] <- "y"
  }
}

Spam_SMS$congratulation <- "n"
for(i in 1:nrow(Spam_SMS)){
  if(str_detect(Spam_SMS$Message[i], "congrats")  == TRUE){
    Spam_SMS$congratulation[i] <- "y"
  }
  if(str_detect(Spam_SMS$Message[i], "congratulations")  == TRUE){
    Spam_SMS$congratulation[i] <- "y"
  }
}

Spam_SMS$adult <- "n"
for(i in 1:nrow(Spam_SMS)){
  if(str_detect(Spam_SMS$Message[i], "xxx")  == TRUE){
    Spam_SMS$adult[i] <- "y"
  }
  if(str_detect(Spam_SMS$Message[i], "babe")  == TRUE){
    Spam_SMS$adult[i] <- "y"
  }
  if(str_detect(Spam_SMS$Message[i], "naked")  == TRUE){
    Spam_SMS$adult[i] <- "y"
  }
    if(str_detect(Spam_SMS$Message[i], "dirty")  == TRUE){
    Spam_SMS$adult[i] <- "y"
    }
    if(str_detect(Spam_SMS$Message[i], "flirty")  == TRUE){
    Spam_SMS$adult[i] <- "y"
    }
}

Spam_SMS$attention <- "n"
for(i in 1:nrow(Spam_SMS)){
  if(str_detect(Spam_SMS$Message[i], "urgent")  == TRUE){
    Spam_SMS$attention[i] <- "y"
  }
    if(str_detect(Spam_SMS$Message[i], "attention")  == TRUE){
    Spam_SMS$attention[i] <- "y"
    }
    if(str_detect(Spam_SMS$Message[i], "bonus")  == TRUE){
    Spam_SMS$attention[i] <- "y"
      }
    if(str_detect(Spam_SMS$Message[i], "immediately")  == TRUE){
    Spam_SMS$attention[i] <- "y"
  }
}

Spam_SMS$ringtone  <- "n"
for(i in 1:nrow(Spam_SMS)){
  if(str_detect(Spam_SMS$Message[i], "ringtone")  == TRUE){
    Spam_SMS$ringtone[i] <- "y"
  }
}
```

##To make the data ready for text analysis. In this, we use text-mining package (package tm) to manage the documents.
```{r}
library(tm)
library(SnowballC)
library(wordcloud)
library(RColorBrewer)
library(magrittr)

BagOfWords <- Corpus(VectorSource(Spam_SMS$Message))

#To clean corpus.
Clean_BagOfWords <- BagOfWords %>%
                    tm_map(content_transformer(tolower)) %>% #Transofrm to lower case
                    tm_map(removeNumbers) %>%                #Clean by removing numbers
                    tm_map(removeWords, stopwords(kind="en")) %>% #Clean by removing stopwords
                    tm_map(removePunctuation) %>%            #Clean by removing punctuation
                    tm_map(stripWhitespace)                  #Clean by tokenising by striping white space

#Transform corpus into matrix.
TDM = TermDocumentMatrix(Clean_BagOfWords) 
```

## Splitting the data in a ratio of 7:3: 70% to build the predictive model and 30% to test the model. I am splitting the dataset, Corpus(BagOfWords) and the Term Document Matrix(FinalData). 
```{r}
library(lattice)
library(caret)
#Random number generation using set.seed of 1234.
set.seed(1234)

#To create a split formula using which I would split the data into train and test sets.
Split_Formula <- createDataPartition(Spam_SMS$MessageLabel, p=0.7, list=FALSE)

#To split Spam_SMS into training and test sets.
train_data <- Spam_SMS[Split_Formula,]
test_data <- Spam_SMS[-Split_Formula,]

#To split corpus into training and test data.
Corpus_train_data <- Clean_BagOfWords[Split_Formula]
Corpus_test_data <- Clean_BagOfWords[-Split_Formula]

#To split Term Document Matrix into training and test data.
TDM_train_data <- TDM[Split_Formula,]
TDM_test_data <- TDM[-Split_Formula,]
```

##Producing Wordclouds
```{r}
#Using Wordcloud to see frequent words. More the word is frequent, larger the font will be for it. 
pal = brewer.pal(6,"Dark2")
wordcloud(BagOfWords, max.words = 75, random.order = FALSE, scale=c(8,.3), colors = pal)
```

```{r}
#Producing separate wordclouds for Legitimate and Spam messages of train_data.
Spam <- subset(train_data, MessageLabel == "Spam")
Legitimate <- subset(train_data, MessageLabel == "Legitimate")

#Wordcloud for Spam
wordcloud(Spam$Message, max.words = 30, scale=c(3, 0, 5), colors = pal)
```

```{r}
#Wordcloud for Legitimate
wordcloud(Legitimate$Message, max.words = 30, scale=c(3, 0, 5), colors = pal)
```

```{r}
#To build a recursive partitioning decision tree.
library(rpart)
library(rpart.plot)
SMS_Rpart <- rpart(formula = MessageLabel ~ free + winner + congratulation + adult + attention + ringtone, data = train_data, method = "class")

rpart.plot(SMS_Rpart, type = 4, fallen.leaves = FALSE, extra = 4)
```
##This tree reveals that out of all these tokens, the most important token is 'free' and the least important ones being 'adult, ringtone and congratulation'.
```{r}
summary(SMS_Rpart)
```
##Applying Random Forest to plot the importance of each token.
```{r}
library(randomForest)

train_data$MessageLabel %<>% as.factor()
train_data$Message  %<>% as.character()
train_data$free %<>% as.factor()
train_data$winner %<>% as.factor()
train_data$congratulation %<>% as.factor()
train_data$adult  %<>% as.factor()
train_data$attention   %<>% as.factor()
train_data$ringtone %<>% as.factor()

#Applying the formula for Random Forest Algorithm
SMS_RF <- MessageLabel ~ free + winner + congratulation + adult + attention + ringtone
RFSpam_Tree <- randomForest(SMS_RF, data = train_data, ntree=25, proximity = T)

#To plot the Variable Importance Plot.
ImportancePlot <- varImpPlot(RFSpam_Tree, main = "Importance of each Token") 

```
This plot also expresses that the most important token aongst all is 'free', and the least important are 'adult, congratulation and ringtone'.

```{r}
#Importance of each token in a tabular form.
importance(RFSpam_Tree)
```

#To test the above Random Forest Model on test data and check the prediction percentage.
```{r}
test_data$MessageLabel %<>% as.factor()
test_data$Message  %<>% as.character()
test_data$free %<>% as.factor()
test_data$winner %<>% as.factor()
test_data$congratulation %<>% as.factor()
test_data$adult  %<>% as.factor()
test_data$attention   %<>% as.factor()
test_data$ringtone %<>% as.factor()


RFTest <- predict(RFSpam_Tree, newdata =test_data)
TestPredictionTable <- table(RFTest, test_data$MessageLabel)
print(TestPredictionTable)
```
##Prediction percentage for test data.
```{r}
TestPredictability <- sum(RFTest == test_data$MessageLabel)/ length(test_data$MessageLabel)*100
message("Predcitibility Percentage for Test Data is:")
print(TestPredictability)
```
##Producing Heatmap for the above model.
```{r}
library(lattice)
library(caret)
#Converting table for 70-30 split regime into a data frame.
RFSpam_Tree_Table <- as.data.frame(table(predict(RFSpam_Tree, newdata = test_data), test_data$MessageLabel))

#Normalizing data between 0 and 1 using Min-Max Normalization. 
Normalized = (RFSpam_Tree_Table$Freq-min(RFSpam_Tree_Table$Freq))/(max(RFSpam_Tree_Table$Freq) - min(RFSpam_Tree_Table$Freq))
#Replacing data of Freq column with Normalized data.
RFSpam_Tree_Table[,"Freq"] <- Normalized

# ggplotting the contents of table of 70-30 split regime into Heatmap.
Heatmap <- ggplot(RFSpam_Tree_Table)

#Final Plotting step
Heatmap + geom_tile(aes(x=Var1, y=Var2, fill=Freq), colour = "white") + scale_x_discrete(name="Reference Class",  position = "top") + scale_y_discrete(name="Predicted Class", limits = rev(levels(RFSpam_Tree_Table$Var1))) + scale_fill_gradient(breaks=seq(from=-.5, to=4, by=.25), low = "white", high = "steelblue") + labs(fill="Normalized\nFrequency")


```



##Train Support Vector Machine
```{r}
library(e1071)

SMS_SVM <- svm(MessageLabel ~ free + winner + congratulation + adult + attention + ringtone, data = train_data, kernel = "linear", cost = 0.1, gamma = 0.1)
SVMTest <- predict(SMS_SVM, test_data)

confusionMatrix(predict(SMS_SVM, newdata = test_data), test_data$MessageLabel)
```

##Producing heatmaps for the above confusion matrix.
```{r}
SVM_Tree <- as.data.frame(table(predict(SMS_SVM, newdata = test_data), test_data$MessageLabel)) 
#Normalizing data between 0 and 1 using Min-Max Normalization. 
Normalized = (SVM_Tree$Freq-min(SVM_Tree$Freq))/(max(SVM_Tree$Freq) - min(SVM_Tree$Freq))
#Replacing data of Freq column with Normalized data.
SVM_Tree[,"Freq"] <- Normalized
#ggplotting the contents of table of 70-30 split regime into Heatmap.
Heatmap <- ggplot(SVM_Tree)
 
#Final Plotting step
Heatmap + geom_tile(aes(x=Var1, y=Var2, fill=Freq), colour = "white") + scale_x_discrete(name="Reference Class",  position = "top") + scale_y_discrete(name="Predicted Class", limits = rev(levels(SVM_Tree$Var1))) + scale_fill_gradient(breaks=seq(from=-.5, to=4, by=.25), low = "white", high = "steelblue") + labs(fill="Normalized\nFrequency")
```

```{r}
svm.accuracy.table <- as.data.frame(table(test_data$MessageLabel, SVMTest))
print(paste("Accuracy for SVM is:",
            100*round(((svm.accuracy.table$Freq[1]+svm.accuracy.table$Freq[4])/nrow(test_data)), 4),
            "%"))

```

## Logistic regression

```{r}
SMS_GLM <- glm(MessageLabel ~ free + winner + congratulation + adult + attention + ringtone, data = train_data, family = "binomial")
GLMTest <- predict(SMS_GLM, test_data)
```

```{r}
library(ROCR)
Logistic_Regression_Prediction <- prediction(abs(GLMTest), test_data$MessageLabel)
Logistic_Regression_Performance <- performance(Logistic_Regression_Prediction,"tpr","fpr")
plot(Logistic_Regression_Performance)
```

```{r}
#table(test_data$Label, Logistic_Regression_Test > 0.75)
glm.accuracy.table <- as.data.frame(table(test_data$MessageLabel, GLMTest > 0.75))
print(paste("Accuracy of Logistic Regression is:",
            100*round(((glm.accuracy.table$Freq[1]+glm.accuracy.table$Freq[4])/nrow(test_data)), 4),
            "%"))
```

##Naive Bayes Classifier
```{r}
#Retain words which appear in 5 or more than 5 SMS messages.
Frequent_Terms = findFreqTerms(TDM_train_data, 5)
TDM_train_data_New = DocumentTermMatrix(Corpus_train_data, list(dictionary=Frequent_Terms))
TDM_test_data_New =  DocumentTermMatrix(Corpus_test_data, list(dictionary=Frequent_Terms))
```

```{r}
#To write a fucntion to convert numerics in TDms to factors of yes/no.
Convert_Numerics_To_Factors = function(num) 
  {
  num = ifelse(num > 0, 1, 0)
  num = factor(num, levels = c(0, 1), labels=c("No", "Yes"))
  return (num)
  }

#Apply above fucntion to the new TDM train and test datasets.
TDM_train_data_New = apply(TDM_train_data_New, MARGIN=2, Convert_Numerics_To_Factors)
TDM_test_data_New  = apply(TDM_test_data_New, MARGIN=2, Convert_Numerics_To_Factors)
```

```{r}
#Train a Naive Bayes Model.

library(e1071)
# store our model in sms_classifier
SMS_NB = naiveBayes(MessageLabel ~., data = train_data, laplace = 1)
SMS_NBTest = predict(SMS_NB, TDM_test_data_New)


library(gmodels)
CrossTable(SMS_NBTest, test_data$MessageLabel, 
           prop.chisq = FALSE, 
           prop.t = FALSE, 
           dnn = c("predicted", "actual")) #Name of column
```