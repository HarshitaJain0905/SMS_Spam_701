---
title: "SMS"
output:
  word_document: default
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:
## Import all the libraries
```{r}
importlib <- c("ggplot2", "stringr", "magrittr", "futile.logger", "VennDiagram", "tm", "SnowballC", "wordcloud", "RColorBrewer", "lattice", "caret", "rpart", "rpart.plot", "randomForest", "e1071", "ROCR", "gmodels")

require(importlib)

lapply(importlib, require, character.only = TRUE)
```

## Download the dataset.
```{r}
Spam_SMS <- read.csv("./SMS_Spam_Dataset.csv", stringsAsFactors = F)
str(Spam_SMS)
```

## Clean the data. 

```{r}
# Remove Null Columns.
Spam_SMS$X <- NULL
Spam_SMS$X.1 <- NULL
Spam_SMS$X.2 <- NULL

#Assign appropriate names to the columns.
names(Spam_SMS) <- c("MessageLabel","Message")

#Check if any other NULL values exist in the dataset.
colSums(is.na(Spam_SMS))

#Convert class into factor.
levels(as.factor(Spam_SMS$MessageLabel))

#Assign appropriate names to the data entries under Column "Message_Label"
Spam_SMS$MessageLabel[Spam_SMS$MessageLabel == "ham"] <- "Legitimate"
Spam_SMS$MessageLabel[Spam_SMS$MessageLabel == "spam"] <- "Spam"

#Convert class into factor.
Spam_SMS$MessageLabel <- factor(Spam_SMS$MessageLabel)
```

## Explore the data

Explore the distribution of Spam and Legitimate Messages.
```{r}
# Produce a data frame displaying the total number of legitmate messages and spam messages.
Distribution <- as.data.frame(table(Spam_SMS$MessageLabel))

# Calculate percentage for each type of Message Label. 
Distribution$Percentage <- (Distribution$Freq/nrow(Spam_SMS))*100
Distribution$Percentage <- round(Distribution$Percentage, digits = 2)
names(Distribution) <- c("Label", "Total", "Percentage")

# Plot the Distribution.
ggplot(Distribution, aes(x = Label, y = Percentage)) + geom_bar(stat= "identity", width = 0.5, fill = "dark green") + ylim(0,100) + labs(colour = "Percentage", title = "Distribution of Legitimate and Spam Messages") + geom_text(aes(label = Percentage), color = "red", vjust = 0) + xlab("Label")+ ylab("Percentage Value")
```
This plot reveals that 86% of all the SMS messages in the dataset are Legitimate messages, while 13% of them are Spam messages. 

To know the length of each text so as to be able to explore the data more.
```{r}
# Count the number of characters in each Message.
Spam_SMS$MessageLength <- nchar(Spam_SMS$Message)

# Find the maximum length of Legitimate Message.
max(Spam_SMS$MessageLength[Spam_SMS$MessageLabel == "Legitimate"])

# Find the maximum length of Spam Message.
max(Spam_SMS$MessageLength[Spam_SMS$MessageLabel == "Spam"])

# Find the minimum length of Legitimate Message.
min(Spam_SMS$MessageLength[Spam_SMS$MessageLabel == "Legitimate"])

# Find the minimum length of Spam Message.
min(Spam_SMS$MessageLength[Spam_SMS$MessageLabel == "Spam"])
```

Plot the distribution of Legitimate and Spam messages v/s the Message Length.
```{r}
ggplot(Spam_SMS, aes(x = MessageLength, fill = MessageLabel)) +
  theme_bw() +
  geom_histogram(binwidth = 5) +
  labs(y = "Number of Messages", x = "Length of Message",
       title = "Distribution of Message Lengths with Class Labels")
```
This plot helps us understand the following:
1. The length of legitimate messages ranges from 2 characters to 910 characters.
2. The length of spam messages ranges from 13 charcters to 224 characters.
3. The most common length of legitimate messages is 22 characters.
4. The most common length of spam messages is 158 characters.

Split Raw SMS Data on Labels (Spam and Legitmate) and produce wordclouds for each. Using Wordcloud would help understand frequent words. More frequent the word, larger the font will be for it. Producing wordclouds would give a better understanding of all the features that differentiate Spam SMSs from Legitimate SMSs.

```{r}
# Splitting Raw SMS Data on Labels (Spam and Legitmate). 
Spam_Raw <- subset(Spam_SMS, MessageLabel == "Spam")
Legitimate_Raw <- subset(Spam_SMS, MessageLabel == "Legitimate")

# Produce wordcloud for Spam_Raw
pal = brewer.pal(6,"Dark2")
wordcloud(Spam_Raw$Message, max.words = 30, scale=c(8, .3), colors = pal)
```
The wordcloud reveals that the most frequent words in Spam messages are: Call, Free, Now, Mobile, Text, Prize.

```{r}
# Produce wordcloud for Legitimate_Raw
wordcloud(Legitimate_Raw$Message, max.words = 30, scale=c(5, .3), colors = pal)
```
The wordcloud reveals that the most frequent words in legitimate messages are: Can, Will, Now, Just, etc.

To convert all the tokens to lower case. Post that, run for loops for words manually selected as differentiating features for Spam SMSs and for words revealed frequent by the above wordcloud produced for spam messages to correctly assigned 'y' or 'n' for each message in the dataset. ('y' corresponds to availability of that word in a particular SMS while 'n' corresponds to non-availability of that word in the SMS) 
```{r}
# Transformation of all tokens to lower case.
Spam_SMS$Message %<>% str_to_lower()

# For loop for token 'free'
Spam_SMS$free <- "n"
for(i in 1:nrow(Spam_SMS)){
  if(str_detect(Spam_SMS$Message[i], "free")  == TRUE){
    Spam_SMS$free[i] <- "y"
  }
}

# For loop for token 'winner, win, won, award, selected'
Spam_SMS$winner <- "n"
for(i in 1:nrow(Spam_SMS)){
  if(str_detect(Spam_SMS$Message[i], "winner")  == TRUE){
    Spam_SMS$winner[i] <- "y"
  }
  if(str_detect(Spam_SMS$Message[i], "win")  == TRUE){
   Spam_SMS$winner[i] <- "y"
  }
  if(str_detect(Spam_SMS$Message[i], "won")  == TRUE){
   Spam_SMS$winner[i] <- "y"
  }
  if(str_detect(Spam_SMS$Message[i], "award")  == TRUE){
   Spam_SMS$winner[i] <- "y"
    }
    if(str_detect(Spam_SMS$Message[i], "selected")  == TRUE){
   Spam_SMS$winner[i] <- "y"
    }
  if(str_detect(Spam_SMS$Message[i], "prize")  == TRUE){
   Spam_SMS$winner[i] <- "y"
  }
  if(str_detect(Spam_SMS$Message[i], "claim")  == TRUE){
   Spam_SMS$winner[i] <- "y"
  }
}

# For loop for token 'congratulations, congrats'
Spam_SMS$congratulation <- "n"
for(i in 1:nrow(Spam_SMS)){
  if(str_detect(Spam_SMS$Message[i], "congrats")  == TRUE){
    Spam_SMS$congratulation[i] <- "y"
  }
  if(str_detect(Spam_SMS$Message[i], "congratulations")  == TRUE){
    Spam_SMS$congratulation[i] <- "y"
  }
}

# For loop for token 'xxx, babe, naked, dirty, flirty'
Spam_SMS$adult <- "n"
for(i in 1:nrow(Spam_SMS)){
  if(str_detect(Spam_SMS$Message[i], "xxx")  == TRUE){
    Spam_SMS$adult[i] <- "y"
  }
  if(str_detect(Spam_SMS$Message[i], "babe")  == TRUE){
    Spam_SMS$adult[i] <- "y"
  }
  if(str_detect(Spam_SMS$Message[i], "naked")  == TRUE){
    Spam_SMS$adult[i] <- "y"
  }
    if(str_detect(Spam_SMS$Message[i], "dirty")  == TRUE){
    Spam_SMS$adult[i] <- "y"
    }
    if(str_detect(Spam_SMS$Message[i], "flirty")  == TRUE){
    Spam_SMS$adult[i] <- "y"
    }
}

# For loop for token 'urgent, attention, bonus, immediately'
Spam_SMS$attention <- "n"
for(i in 1:nrow(Spam_SMS)){
  if(str_detect(Spam_SMS$Message[i], "urgent")  == TRUE){
    Spam_SMS$attention[i] <- "y"
  }
    if(str_detect(Spam_SMS$Message[i], "attention")  == TRUE){
    Spam_SMS$attention[i] <- "y"
    }
    if(str_detect(Spam_SMS$Message[i], "bonus")  == TRUE){
    Spam_SMS$attention[i] <- "y"
      }
    if(str_detect(Spam_SMS$Message[i], "immediately")  == TRUE){
    Spam_SMS$attention[i] <- "y"
    }
  if(str_detect(Spam_SMS$Message[i], "now")  == TRUE){
   Spam_SMS$attention[i] <- "y"
  }
  if(str_detect(Spam_SMS$Message[i], "stop")  == TRUE){
   Spam_SMS$attention[i] <- "y"
  }
}

# For loop for token 'ringtone'
Spam_SMS$ringtone  <- "n"
for(i in 1:nrow(Spam_SMS)){
  if(str_detect(Spam_SMS$Message[i], "ringtone")  == TRUE){
    Spam_SMS$ringtone[i] <- "y"
  }
  if(str_detect(Spam_SMS$Message[i], "call")  == TRUE){
   Spam_SMS$ringtone[i] <- "y"
  }
  if(str_detect(Spam_SMS$Message[i], "mobile")  == TRUE){
   Spam_SMS$ringtone[i] <- "y"
  }
  if(str_detect(Spam_SMS$Message[i], "text")  == TRUE){
   Spam_SMS$ringtone[i] <- "y"
  }
  if(str_detect(Spam_SMS$Message[i], "txt")  == TRUE){
   Spam_SMS$ringtone[i] <- "y"
  }
}

```
After having this chunk run, there are 6 more columns appended to the dataset (Spam_SMS) with values = y or n, depending on the availability of the keywords in messages. 

Plot bar graph depicting total number of messages with the value of these features being equal to "y".
```{r}
#For Unigrams

# Produce a data frame 'Spam_Features' containing Features and the total number of messages containing that feature.
Spam_Features <- data.frame(Features = c("Free", "Adult", "Ringtone", "Congratulation", "Winner", "Attention"), Total = c(sum(Spam_SMS$free == "y"), sum(Spam_SMS$adult == "y"), sum(Spam_SMS$ringtone == "y"), sum(Spam_SMS$congratulation == "y"), sum(Spam_SMS$winner == "y"), sum(Spam_SMS$attention == "y")))

# Plot the data frame.
ggplot(Spam_Features, aes(x = reorder(Features, -Total), y = Total)) + geom_bar(stat = "identity", fill = "steelblue") + geom_text(aes(label = Total), color = "red", vjust = 0) + xlab("Features")+ ylab("Total Number of Messages")
```
The plot reveals that the most important features for Spam SMS in this dataset are : Ringtone, Attention and Winner, while the least important are: Congratulations, Adult and Free. 

Produce Venn Diagram to analyse how many SMS messages have bigrams' feature combination and trigrams' feature combinations.
```{r}
# Compute the number of SMS messages having combination of two and/or three features. After having obtained these values, Venn Diagrams would be produced for these combinations.

#For bigrams

#For Free and Adult
Free_Adult <- sum(Spam_SMS$free == "y" & Spam_SMS$adult == "y")
Free_Adult
# Venn Diagram for the bigram
grid.newpage()
draw.pairwise.venn(area1 = 265, area2 = 150, cross.area = 9, category = c("Free", 
    "Adult"), lty = rep("blank", 
    2), fill = c("light blue", "pink"), alpha = rep(0.5, 2), cat.pos = c(0, 
    0), cat.dist = rep(0.025, 2))

#For Free and Ringtone
Free_Ringtone <- sum(Spam_SMS$free == "y" & Spam_SMS$ringtone == "y")
Free_Ringtone
# Venn Diagram for the bigram
grid.newpage()
draw.pairwise.venn(area1 = 265, area2 = 994, cross.area = 193, category = c("Free", 
    "Ringtone"), lty = rep("blank", 
    2), fill = c("light blue", "pink"), alpha = rep(0.5, 2), cat.pos = c(0, 
    0), cat.dist = rep(0.025, 2))

#For Free and Congratulation
Free_Congratulation <- sum(Spam_SMS$free == "y" & Spam_SMS$congratulation == "y")
Free_Congratulation
# Venn Diagram for the bigram
grid.newpage()
draw.pairwise.venn(area1 = 265, area2 = 34, cross.area = 9, category = c("Free", 
    "Congratulation"), lty = rep("blank", 
    2), fill = c("light blue", "pink"), alpha = rep(0.5, 2), cat.pos = c(0, 
    0), cat.dist = rep(0.025, 2))

#For Free and Winner
Free_Winner <- sum(Spam_SMS$free == "y" & Spam_SMS$winner == "y")
Free_Winner
# Venn Diagram for the bigram
grid.newpage()
draw.pairwise.venn(area1 = 265, area2 = 419, cross.area = 52, category = c("Free", 
    "Winner"), lty = rep("blank", 
    2), fill = c("light blue", "pink"), alpha = rep(0.5, 2), cat.pos = c(0, 
    0), cat.dist = rep(0.025, 2))

#For Free and Attention
Free_Attention <- sum(Spam_SMS$free == "y" & Spam_SMS$attention == "y")
Free_Attention
# Venn Diagram for the bigram
grid.newpage()
draw.pairwise.venn(area1 = 265, area2 = 928, cross.area = 104, category = c("Free", 
    "Attention"), lty = rep("blank", 
    2), fill = c("light blue", "pink"), alpha = rep(0.5, 2), cat.pos = c(0, 
    0), cat.dist = rep(0.025, 2))

#For Adult and Winner
Adult_Winner <- sum(Spam_SMS$adult == "y" & Spam_SMS$winner == "y")
Adult_Winner
# Venn Diagram for the bigram
grid.newpage()
draw.pairwise.venn(area1 = 150, area2 = 419, cross.area = 9, category = c("Adult", 
    "Winner"), lty = rep("blank", 
    2), fill = c("light blue", "pink"), alpha = rep(0.5, 2), cat.pos = c(0, 
    0), cat.dist = rep(0.025, 2))

#For Adult and Attention
Adult_Attention <- sum(Spam_SMS$adult == "y" & Spam_SMS$attention == "y")
Adult_Attention
# Venn Diagram for the bigram
grid.newpage()
draw.pairwise.venn(area1 = 150, area2 = 928, cross.area = 29, category = c("Adult", 
    "Attention"), lty = rep("blank", 
    2), fill = c("light blue", "pink"), alpha = rep(0.5, 2), cat.pos = c(0, 
    0), cat.dist = rep(0.025, 2))

#For Congratulation and Winner
congratulation_Winner <- sum(Spam_SMS$congratulation == "y" & Spam_SMS$winner == "y")
congratulation_Winner
# Venn Diagram for the bigram
grid.newpage()
draw.pairwise.venn(area1 = 34, area2 = 419, cross.area = 14, category = c("Congratulation", 
    "Winner"), lty = rep("blank", 
    2), fill = c("light blue", "pink"), alpha = rep(0.5, 2), cat.pos = c(0, 
    0), cat.dist = rep(0.025, 2))

#For Attention and Winner
Attention_Winner <- sum(Spam_SMS$attention == "y" & Spam_SMS$winner == "y")
Attention_Winner
# Venn Diagram for the bigram
grid.newpage()
draw.pairwise.venn(area1 = 928, area2 = 419, cross.area = 161, category = c("Attention", 
    "Winner"), lty = rep("blank", 
    2), fill = c("light blue", "pink"), alpha = rep(0.5, 2), cat.pos = c(0, 
    0), cat.dist = rep(0.025, 2))

#For Ringtone and Winner
Ringtone_Winner <- sum(Spam_SMS$ringtone == "y" & Spam_SMS$winner == "y")
Ringtone_Winner
# Venn Diagram for the bigram
grid.newpage()
draw.pairwise.venn(area1 = 994, area2 = 419, cross.area = 235, category = c("Ringtone", 
    "Winner"), lty = rep("blank", 
    2), fill = c("light blue", "pink"), alpha = rep(0.5, 2), cat.pos = c(0, 
    0), cat.dist = rep(0.025, 2))

#For Ringtone and Congratulation
Ringtone_Congratulation <- sum(Spam_SMS$ringtone == "y" & Spam_SMS$congratulation == "y")
Ringtone_Congratulation
# Venn Diagram for the bigram
grid.newpage()
draw.pairwise.venn(area1 = 994, area2 = 34, cross.area = 23, category = c("Ringtone", 
    "Congratulation"), lty = rep("blank", 
    2), fill = c("light blue", "pink"), alpha = rep(0.5, 2), cat.pos = c(0, 
    0), cat.dist = rep(0.025, 2))

#For Attention and Congratulation
Attention_Congratulation <- sum(Spam_SMS$attention == "y" & Spam_SMS$congratulation == "y")
Attention_Congratulation
# Venn Diagram for the bigram
grid.newpage()
draw.pairwise.venn(area1 = 928, area2 = 34, cross.area = 15, category = c("Attention", 
    "Congratulation"), lty = rep("blank", 
    2), fill = c("light blue", "pink"), alpha = rep(0.5, 2), cat.pos = c(0, 
    0), cat.dist = rep(0.025, 2))

#For Attention and Ringtone
Attention_Ringtone <- sum(Spam_SMS$attention == "y" & Spam_SMS$ringtone == "y")
Attention_Ringtone
# Venn Diagram for the bigram
grid.newpage()
draw.pairwise.venn(area1 = 928, area2 = 994, cross.area = 368, category = c("Attention", 
    "Ringtone"), lty = rep("blank", 
    2), fill = c("light blue", "pink"), alpha = rep(0.5, 2), cat.pos = c(0, 
    0), cat.dist = rep(0.025, 2))

#For Adult and Ringtone
Adult_Ringtone <- sum(Spam_SMS$adult == "y" & Spam_SMS$ringtone == "y")
Adult_Ringtone
# Venn Diagram for the bigram
grid.newpage()
draw.pairwise.venn(area1 = 150, area2 = 994, cross.area = 39, category = c("Adult", 
    "Ringtone"), lty = rep("blank", 
    2), fill = c("light blue", "pink"), alpha = rep(0.5, 2), cat.pos = c(0, 
    0), cat.dist = rep(0.025, 2))
```

```{r}
#For trigrams

#For free, congratulation and winner 
Free_Congratulation_Winner <- sum(Spam_SMS$free == "y" & Spam_SMS$congratulation == "y" & Spam_SMS$winner == "y")
Free_Congratulation_Winner
# Venn Diagram for the trigram
grid.newpage()
draw.triple.venn(area1 = 265, area2 = 34, area3 = 419, n12 = 9, n23 = 14, n13 = 52, 
    n123 = 6, category = c("Free", "Congratulation", "Winner"), lty = "blank", 
    fill = c("skyblue", "pink1", "mediumorchid"))

#For free, attention and winner 
Free_Attention_Winner <- sum(Spam_SMS$free == "y" & Spam_SMS$attention == "y" & Spam_SMS$winner == "y")
Free_Attention_Winner
# Venn Diagram for the trigram
grid.newpage()
draw.triple.venn(area1 = 265, area2 = 928, area3 = 419, n12 = 104, n23 = 161, n13 = 52, 
    n123 = 2, category = c("Free", "Attention", "Winner"), lty = "blank", 
    fill = c("skyblue", "pink1", "mediumorchid"))

#For adult, attention and winner 
Adult_Attention_Winner <- sum(Spam_SMS$adult == "y" & Spam_SMS$attention == "y" & Spam_SMS$winner == "y")
Adult_Attention_Winner
# Venn Diagram for the trigram
grid.newpage()
draw.triple.venn(area1 = 150, area2 = 928, area3 = 419, n12 = 29, n23 = 161, n13 = 9, 
    n123 = 3, category = c("Adult", "Attention", "Winner"), lty = "blank", 
    fill = c("skyblue", "pink1", "mediumorchid"))
```
## Text Analysis
To make the data ready for text analysis. In this, we use text-mining package (package tm) to manage the documents.
```{r}
# create a Corpus of Messages in Spam_SMS. 
BagOfWords <- Corpus(VectorSource(Spam_SMS$Message))

# Clean corpus.
Clean_BagOfWords <- BagOfWords %>%
                    tm_map(content_transformer(tolower)) %>% #Transofrm to lower case
                    tm_map(removeNumbers) %>%                #Clean by removing numbers
                    tm_map(removeWords, stopwords(kind="en")) %>% #Clean by removing stopwords
                    tm_map(removePunctuation) %>%            #Clean by removing punctuation
                    tm_map(stripWhitespace)                  #Clean by tokenising by striping white space

# Transform corpus into matrix.
TDM = DocumentTermMatrix(Clean_BagOfWords)

SparseWords <- removeSparseTerms(TDM, 0.995)

# Transform the matrix of Sparsewords into data frame.
SparseWords <- as.data.frame(as.matrix(SparseWords))

# Rename column names.
colnames(SparseWords) <- make.names(colnames(SparseWords))

str(SparseWords)

SparseWords$MessageLabel <- Spam_SMS$MessageLabel

```
## Classification Process to accurately classify SMS messages into Spam messages or Legitimate messages.

Splitting the data in a ratio of 7:3: 70% to build the predictive model and 30% to test the model. I am splitting the dataset, Sparsewords, Corpus(BagOfWords) and the Term Document Matrix(FinalData). 
```{r}
# Random number generation using set.seed of 1234.
set.seed(1234)

# Create a split formula using which I would split the data into train and test sets.
Split_Formula <- createDataPartition(Spam_SMS$MessageLabel, p=0.7, list=FALSE)

# Split Spam_SMS into training and test sets.
train_data <- Spam_SMS[Split_Formula,]
test_data <- Spam_SMS[-Split_Formula,]

# Split SparseWords into training and test sets.
Sparse_train_data <- SparseWords[Split_Formula,]
Sparse_test_data <- SparseWords[-Split_Formula,]

# Split corpus into training and test data.
Corpus_train_data <- Clean_BagOfWords[Split_Formula]
Corpus_test_data <- Clean_BagOfWords[-Split_Formula]

# Split Term Document Matrix into training and test data.
TDM_train_data <- TDM[Split_Formula,]
TDM_test_data <- TDM[-Split_Formula,]


```

Producing Wordcloud of the cleaned Corpus for analysis.
```{r}
wordcloud(Clean_BagOfWords, max.words = 75, random.order = FALSE, scale=c(5, .3), colors = pal)
```
The wordcloud reveals that the most frequent words in Clean Corpus(mix of Legitimate and Spam messages) are: Call, Can, Now, Get, Just, Will, Free, etc. Therefore, it is evident that this wordcloud substantiates the two wordclouds produced above (each for spam an legitimate messages) as this wordcloud has a mix of the frequent words shown in those wordclouds (like: Free, Call, Can, Just)

Split train_data on Labels (Spam and Legitmate) and produce wordclouds for each. Using Wordcloud would help understand frequent words. More frequent the word, larger the font will be for it.
```{r}
# Splitting train_data on Labels (Spam and Legitmate).
Spam <- subset(train_data, MessageLabel == "Spam")
Legitimate <- subset(train_data, MessageLabel == "Legitimate")

# Produce wordcloud for Spam
wordcloud(Spam$Message, max.words = 30, scale=c(7, .3), colors = pal)
```
The wordcloud reveals that the most frequent words in Spam messages for train data are: Call, Free, Now, Claim. Text, etc. they are the same as the ones displayed in the wordcloud for Spam messages in Spam_SMS dataset.


```{r}
# Produce wordcloud for Legitimate.
wordcloud(Legitimate$Message, max.words = 30, scale=c(5, .3), colors = pal)
```
The wordcloud reveals that the most frequent words in Legitimate messages for train data are: Will, Can, Now, Just, etc. they are the same as the ones displayed in the wordcloud for Legitimate messages in Spam_SMS dataset.

## Building models based on the manually selected 6 features of Spam SMS.

## Decision Tree Model
```{r}
#To build a recursive partitioning decision tree.
SMS_Rpart <- rpart(formula = MessageLabel ~ free + winner + congratulation + adult + attention + ringtone, data = train_data, method = "class")

rpart.plot(SMS_Rpart, type = 4, fallen.leaves = FALSE, extra = 4)
```
This tree reveals that out of all these tokens, the most important token is 'free' and the least important ones being 'adult, ringtone and congratulation'.

```{r}
summary(SMS_Rpart)
```
## Randome Forest Classifier

Apply Random Forest to plot the importance of each token.
```{r}
train_data$MessageLabel %<>% as.factor()
train_data$Message  %<>% as.character()
train_data$free %<>% as.factor()
train_data$winner %<>% as.factor()
train_data$congratulation %<>% as.factor()
train_data$adult  %<>% as.factor()
train_data$attention   %<>% as.factor()
train_data$ringtone %<>% as.factor()

# Apply the formula for Random Forest Algorithm
SMS_RF <- MessageLabel ~ free + winner + congratulation + adult + attention + ringtone
RFSpam_Tree <- randomForest(SMS_RF, data = train_data, ntree=25, proximity = T)

# Plot the Variable Importance Plot.
ImportancePlot <- varImpPlot(RFSpam_Tree, main = "Importance of each Token") 

```
This plot also expresses that the most important token aongst all is 'free', and the least important are 'adult, congratulation and ringtone'.

```{r}
#Importance of each token in a tabular form.
importance(RFSpam_Tree)
```

#To test the above Random Forest Model on test data and check the prediction percentage.
```{r}
test_data$MessageLabel %<>% as.factor()
test_data$Message  %<>% as.character()
test_data$free %<>% as.factor()
test_data$winner %<>% as.factor()
test_data$congratulation %<>% as.factor()
test_data$adult  %<>% as.factor()
test_data$attention   %<>% as.factor()
test_data$ringtone %<>% as.factor()


RFTest <- predict(RFSpam_Tree, newdata =test_data)
TestPredictionTable <- table(RFTest, test_data$MessageLabel)
print(TestPredictionTable)
```

Confusion Matrix
```{r}
Measure <- confusionMatrix(predict(RFSpam_Tree, newdata = test_data), test_data$MessageLabel)
Measure$byClass[5:7]
```
##Prediction percentage for test data.
```{r}
TestPredictability <- sum(RFTest == test_data$MessageLabel)/ length(test_data$MessageLabel)*100
message("Predcitability Percentage for Test Data is:")
print(TestPredictability)
```
##Producing Heatmap for the above model.
```{r}
library(lattice)
library(caret)
#Converting table for 70-30 split regime into a data frame.
RFSpam_Tree_Table <- as.data.frame(table(predict(RFSpam_Tree, newdata = test_data), test_data$MessageLabel))

#Normalizing data between 0 and 1 using Min-Max Normalization. 
Normalized = (RFSpam_Tree_Table$Freq-min(RFSpam_Tree_Table$Freq))/(max(RFSpam_Tree_Table$Freq) - min(RFSpam_Tree_Table$Freq))
#Replacing data of Freq column with Normalized data.
RFSpam_Tree_Table[,"Freq"] <- Normalized

# ggplotting the contents of table of 70-30 split regime into Heatmap.
Heatmap <- ggplot(RFSpam_Tree_Table)

#Final Plotting step
Heatmap + geom_tile(aes(x=Var1, y=Var2, fill=Freq), colour = "white") + scale_x_discrete(name="Reference Class",  position = "top") + scale_y_discrete(name="Predicted Class", limits = rev(levels(RFSpam_Tree_Table$Var1))) + scale_fill_gradient(breaks=seq(from=-.5, to=4, by=.25), low = "white", high = "steelblue") + labs(fill="Normalized\nFrequency")


```



##Train Support Vector Machine
```{r}
SMS_SVM <- svm(MessageLabel ~ free + winner + congratulation + adult + attention + ringtone, data = train_data, kernel = "linear", cost = 0.1, gamma = 0.1)
SVMTest <- predict(SMS_SVM, test_data)

SVM_Measure <- confusionMatrix(predict(SMS_SVM, newdata = test_data), test_data$MessageLabel)
SVM_Measure$byClass[5:7]
```

##Producing heatmaps for the above confusion matrix.
```{r}
SVM_Tree <- as.data.frame(table(predict(SMS_SVM, newdata = test_data), test_data$MessageLabel)) 
#Normalizing data between 0 and 1 using Min-Max Normalization. 
Normalized = (SVM_Tree$Freq-min(SVM_Tree$Freq))/(max(SVM_Tree$Freq) - min(SVM_Tree$Freq))
#Replacing data of Freq column with Normalized data.
SVM_Tree[,"Freq"] <- Normalized
#ggplotting the contents of table of 70-30 split regime into Heatmap.
Heatmap <- ggplot(SVM_Tree)
 
#Final Plotting step
Heatmap + geom_tile(aes(x=Var1, y=Var2, fill=Freq), colour = "white") + scale_x_discrete(name="Reference Class",  position = "top") + scale_y_discrete(name="Predicted Class", limits = rev(levels(SVM_Tree$Var1))) + scale_fill_gradient(breaks=seq(from=-.5, to=4, by=.25), low = "white", high = "steelblue") + labs(fill="Normalized\nFrequency")
```

```{r}
svm.accuracy.table <- as.data.frame(table(test_data$MessageLabel, SVMTest))
print(paste("Accuracy for SVM is:",
            100*round(((svm.accuracy.table$Freq[1]+svm.accuracy.table$Freq[4])/nrow(test_data)), 4),
            "%"))

```

## Logistic regression

```{r}
SMS_GLM <- glm(MessageLabel ~ free + winner + congratulation + adult + attention + ringtone, data = train_data, family = "binomial")
GLMTest <- predict(SMS_GLM, test_data, type = 'response')

#Confusion Matrix
table(test_data$MessageLabel, GLMTest > 0.5)
```

```{r}
library(ROCR)
Logistic_Regression_Prediction <- prediction(abs(GLMTest), test_data$MessageLabel)
Logistic_Regression_Performance <- performance(Logistic_Regression_Prediction,"tpr","fpr")
plot(Logistic_Regression_Performance, colorize = TRUE, text.adj = c(-0.2,1.7))
```

```{r}
#table(test_data$Label, Logistic_Regression_Test > 0.75)
glm.accuracy.table <- as.data.frame(table(test_data$MessageLabel, GLMTest > 0.75))
print(paste("Accuracy of Logistic Regression is:",
            100*round(((glm.accuracy.table$Freq[1]+glm.accuracy.table$Freq[4])/nrow(test_data)), 4),
            "%"))
```

##Naive Bayes Classifier
```{r}
#Retain words which appear in 5 or more than 5 SMS messages.
Frequent_Terms = findFreqTerms(TDM_train_data, 5)
TDM_train_data_New = DocumentTermMatrix(Corpus_train_data, list(dictionary=Frequent_Terms))
TDM_test_data_New =  DocumentTermMatrix(Corpus_test_data, list(dictionary=Frequent_Terms))
```

```{r}
#To write a function to convert numerics in TDms to factors of yes/no.
Convert_Numerics_To_Factors = function(num) 
  {
  num = ifelse(num > 0, 1, 0)
  num = factor(num, levels = c(0, 1), labels=c("No", "Yes"))
  return (num)
  }

#Apply above fucntion to the new TDM train and test datasets.
TDM_train_data_New = apply(TDM_train_data_New, MARGIN=2, Convert_Numerics_To_Factors)
TDM_test_data_New  = apply(TDM_test_data_New, MARGIN=2, Convert_Numerics_To_Factors)
```

```{r}
#Train a Naive Bayes Model.

#library(e1071)
# store our model in sms_classifier
SMS_NB = naiveBayes(MessageLabel ~ free + winner + congratulation + adult + attention + ringtone, data = train_data, laplace = 1)
SMS_NBTest = predict(SMS_NB, TDM_test_data_New)


library(gmodels)
CT <- CrossTable(SMS_NBTest, test_data$MessageLabel, 
           prop.chisq = FALSE, 
           prop.t = FALSE, 
           dnn = c("Predicted", "Actual")) #Name of column
```
Logistic Regression
```{r}
SMS_GLM_Blah <- glm(MessageLabel ~., data = Sparse_train_data, family = "binomial")
GLMTest_Blah <- predict(SMS_GLM_Blah, Sparse_test_data, type = 'response')

#Confusion Matrix
table(Sparse_test_data$MessageLabel, GLMTest_Blah > 0.5)
```

```{r}
library(ROCR)
Logistic_Regression_Prediction_Blah <- prediction(abs(GLMTest_Blah), Sparse_test_data$MessageLabel)
Logistic_Regression_Performance_Blah <- performance(Logistic_Regression_Prediction_Blah,"tpr","fpr")

probab.cuts <- data.frame(cut=Logistic_Regression_Performance_Blah@alpha.values[[1]], prec=Logistic_Regression_Performance_Blah@y.values[[1]], rec=Logistic_Regression_Performance_Blah@x.values[[1]])
tail(probab.cuts[probab.cuts$cut > 0.5,], 1)
plot(Logistic_Regression_Performance_Blah, colorize = TRUE, text.adj = c(-0.2,1.7))
```

